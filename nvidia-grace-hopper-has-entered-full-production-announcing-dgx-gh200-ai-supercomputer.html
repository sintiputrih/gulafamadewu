<!doctype html><html lang=en itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=robots content="index,follow,noarchive"><title>Grace Hopper Has Entered Full Production &amp;amp; Announcing DGX GH200 AI Supercomputer - PicoD</title><meta name=description content="Teeing off an AI-heavy slate of announcements for NVIDIA, the company has confirmed that their Grace Hopper superchip has entered full production. The combination of a Grace CPU and Hopper H100 GPU, Grace Hopper is designed to be NVIDIAs answer for customers who need a more tightly integrated CPU + GPU solution for their workloads"><meta name=author content="Some Person"><script type=application/ld+json>{"@context":"http://schema.org","@type":"WebSite","name":"PicoD","url":"\/"}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"Organization","name":"","url":"\/"}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"\/","name":"home"}},{"@type":"ListItem","position":3,"item":{"@id":"\/nvidia-grace-hopper-has-entered-full-production-announcing-dgx-gh200-ai-supercomputer.html","name":"Grace hopper has entered full production \u0026amp; announcing dgx gh200 ai supercomputer"}}]}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"Article","author":{"name":"Aldo Pusey"},"headline":"Grace Hopper Has Entered Full Production \u0026amp; Announcing DGX GH200 AI Supercomputer","description":"Teeing off an AI-heavy slate of announcements for NVIDIA, the company has confirmed that their Grace Hopper superchip has entered full production. The combination of a Grace CPU and Hopper H100 GPU, Grace Hopper is designed to be NVIDIAs answer for customers who need a more tightly integrated CPU \u002b GPU solution for their workloads","inLanguage":"en","wordCount":1350,"datePublished":"2024-10-08T00:00:00","dateModified":"2024-10-08T00:00:00","image":"\/img\/avatar-icon.png","keywords":[""],"mainEntityOfPage":"\/nvidia-grace-hopper-has-entered-full-production-announcing-dgx-gh200-ai-supercomputer.html","publisher":{"@type":"Organization","name":"\/","logo":{"@type":"ImageObject","url":"\/img\/avatar-icon.png","height":60,"width":60}}}</script><meta property="og:title" content="Grace Hopper Has Entered Full Production &amp;amp; Announcing DGX GH200 AI Supercomputer"><meta property="og:description" content="Teeing off an AI-heavy slate of announcements for NVIDIA, the company has confirmed that their Grace Hopper superchip has entered full production. The combination of a Grace CPU and Hopper H100 GPU, Grace Hopper is designed to be NVIDIAs answer for customers who need a more tightly integrated CPU + GPU solution for their workloads"><meta property="og:image" content="/img/avatar-icon.png"><meta property="og:url" content="/nvidia-grace-hopper-has-entered-full-production-announcing-dgx-gh200-ai-supercomputer.html"><meta property="og:type" content="website"><meta property="og:site_name" content="PicoD"><meta name=twitter:title content="Grace Hopper Has Entered Full Production &amp;amp; Announcing DGX GH200 AI …"><meta name=twitter:description content="Teeing off an AI-heavy slate of announcements for NVIDIA, the company has confirmed that their Grace Hopper superchip has entered full production. The combination of a Grace CPU and Hopper H100 GPU, …"><meta name=twitter:image content="/img/avatar-icon.png"><meta name=twitter:card content="summary"><meta name=twitter:site content="@username"><meta name=twitter:creator content="@username"><link href=./img/favicon.ico rel=icon type=image/x-icon><meta name=generator content="Hugo 0.98.0"><link rel=alternate href=./index.xml type=application/rss+xml title=PicoD><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css integrity=sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.5.0/css/all.css integrity=sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU crossorigin=anonymous><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css integrity=sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u crossorigin=anonymous><link rel=stylesheet href=https://assets.cdnweb.info/hugo/bh/css/main.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic"><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800"><link rel=stylesheet href=https://assets.cdnweb.info/hugo/bh/css/highlight.min.css><link rel=stylesheet href=https://assets.cdnweb.info/hugo/bh/css/codeblock.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css integrity=sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css integrity=sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R crossorigin=anonymous></head><body><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header class=header-section><div class="intro-header no-img"><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><h1>Grace Hopper Has Entered Full Production &amp;amp; Announcing DGX GH200 AI Supercomputer</h1><span class=post-meta><i class="fas fa-calendar"></i>&nbsp;Posted on October 8, 2024
&nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;7&nbsp;minutes
&nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;1350&nbsp;words
&nbsp;|&nbsp;<i class="fas fa-user"></i>&nbsp;Aldo Pusey</span></div></div></div></div></div></header><div class=container role=main><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><article role=main class=blog-post><p>Teeing off an AI-heavy slate of announcements for NVIDIA, the company has confirmed that their Grace Hopper “superchip” has entered full production. The combination of a Grace CPU and Hopper H100 GPU, Grace Hopper is designed to be NVIDIA’s answer for customers who need a more tightly integrated CPU + GPU solution for their workloads – particularly for AI models.</p><p>In the works for a few years now, Grace Hopper is NVIDIA’s efforts to leverage both their existing strength in the GPU space and newfound efforts in the CPU space to deliver a semi-integrated CPU/GPU product unlike anything their top-line competitors offer. With NVIDIA’s traditional dominance in the GPU space, the company has essentially been working backwards, combining their GPU technology with other types of processors (CPUs, DPUs, etc) in order to access markets that benefit from GPU acceleration, but where fully discrete GPUs may not be the best solution.</p><table align=center border=1 cellpadding=3 cellspacing=0 width=550><tbody readability=1><tr class=tgrey readability=2><td align=center colspan=4>NVIDIA Grace Hopper Specifications</td></tr><tr class=tlblue><td align=center class=contentwhite width=243>&nbsp;</td><td align=center class=contentwhite width=389>Grace Hopper (GH200)</td></tr><tr><td align=left class=tlgrey><strong>CPU Cores</strong></td><td align=center>72</td></tr><tr><td align=left class=tlgrey><strong>CPU Architecture</strong></td><td align=center>Arm Neoverse V2</td></tr><tr><td align=left class=tlgrey><strong>CPU Memory Capacity</strong></td><td align=center>&lt;=480GB LPDDR5X (ECC)</td></tr><tr><td align=left class=tlgrey><strong>CPU Memory Bandwidth</strong></td><td align=center>&lt;=512GB/sec</td></tr><tr><td align=left class=tlgrey><strong>GPU SMs</strong></td><td align=center>132</td></tr><tr><td align=left class=tlgrey><strong>GPU Tensor Cores</strong></td><td align=center>528</td></tr><tr><td align=left class=tlgrey><strong>GPU Architecture</strong></td><td align=center>Hopper</td></tr><tr><td align=left class=tlgrey><strong>GPU Memory Capcity</strong></td><td align=center>&lt;=96GB</td></tr><tr><td align=left class=tlgrey><strong>GPU Memory Bandwidth</strong></td><td align=center>&lt;=4TB/sec</td></tr><tr><td align=left class=tlgrey><strong>GPU-to-CPU Interface</strong></td><td align=center>900GB/sec<br>NVLink 4</td></tr><tr><td align=left class=tlgrey><strong>TDP</strong></td><td align=center>450W - 1000W</td></tr><tr><td align=left class=tlgrey><strong>Manufacturing Process</strong></td><td align=center>TSMC 4N</td></tr><tr><td align=left class=tlgrey><strong>Interface</strong></td><td align=center>Superchip</td></tr></tbody></table><p>In this first NVIDIA HPC CPU + GPU mash-up, the Hopper GPU is the known side of the equation. While it only started shipping in appreciable volumes this year, NVIDIA was <a href=#>detailing the Hopper architecture and performance expectations over a year ago</a>. Based on the 80B transistor GH100 GPU, H100 brings just shy of 1 PFLOPS of FP16 matrix math throughput for AI workloads, as well as 80GB of HBM3 memory. H100 is itself already a huge success – thanks to the explosion of ChatGPT and other generative AI services, NVIDIA is already selling everything they can make – but NVIDIA is still pushing ahead with their efforts to break into markets where the workloads require closer CPU/GPU integration.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/18877/nvidia-grace-hopper-superchip_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>Being paired with H100, in turn, is NVIDIA’s <a href=#>Grace CPU</a>, which itself just entered full production a couple of months ago. The Arm Neoverse V2-based chip packs 72 CPU cores, and comes with up to 480GB of LPDDR5X memory. And while the CPU cores are themselves plenty interesting, the bigger twist with Grace has been NVIDIA’s decision to co-package the CPU with LPDDR5X, rather than using slotted DIMMs. The on-package memory has allowed NVIDIA to use both higher clocked and lower power memory – at the cost of expandability – which makes Grace unlike any other HPC-class CPU on the market. And potentially a very big deal for Large Language Model (LLM) training, given the emphasis on both dataset sizes and the memory bandwidth needed to shuffle that data around.</p><p>It’s that data shuffling, in turn, that helps to define a single Grace Hopper board as something more than just a CPU and GPU glued together on the same board. Because NVIDIA equipped Grace with NVLink support – NVIDIA’s proprietary high-bandwidth chip interconnect – Grace and Hopper have a much faster interconnect than a traditional, PCIe-based CPU + GPU setup. The resulting NVLink Chip-to-Chip (C2C) link offers 900GB/second of bandwidth between the two chips (450GB/sec in each direction), giving Hopper the ability to talk back to Grace even faster than Grace can read or write to its own memory.</p><p>The resulting board, which NVIDIA calls their GH200 “superchip”, is meant to be NVIDIA’s answer to the AI and HPC markets for the next product cycle. For customers who need a more local CPU than a traditional CPU + GPU setup – or perhaps more pointedly, more quasi-local memory than a stand-alone GPU can be equipped with – Grace Hopper is NVIDIA’s most comprehensive compute product yet. Meanwhile, with there being some uncertainty over just how prevalent the Grace-only (CPU-only) superchip will be, given that NVIDIA is currently on an AI bender, Grace Hopper may very well end up being where we see the most of Grace, as well.</p><p>According to NVIDIA, systems incorporating GH200 chips are slated to be available later this year.</p><h2>DGX GH200 AI Supercomputer: Grace Hopper Goes Straight To the Big Leagues</h2><p>Meanwhile, even though Grace Hopper is not technically out the door yet, NVIDIA is already at work building its first DGX system around the chip. Though in this case, “DGX” may be a bit of a misnomer for the system, which unlike other DGX systems isn’t a single node, but rather a full-on multi-rack computational cluster – hence NVIDIA terming it a “supercomputer.”</p><p>At a high level, the DGX GH200 AI Supercomputer is a complete, turn-key, 256 node GH200 cluster. Spanning some 24 racks, a single DGX GH200 contains 256 GH200 chips – and thus, 256 Grace CPUs and 256 H100 GPUs – as well as all of the networking hardware needed to interlink the systems for operation. In cumulative total, a DGX GH200 cluster offers 120TB of CPU-attached memory, another 24TB of GPU-attached memory, and a total of 1 EFLOPS of FP8 throughput (with sparsity).</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/18877/nvidia-dgx-gh200_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a><br>Look Closer: That's Not a Server Node - That's 24 Server Racks</p><p>Linking the nodes together is a two-layer networking system built around NVLink. 96 local, L1 switches provide immediate communications between the GH200 blades, while another 36 L2 switches provide a second layer of connectivity tying together the L1 switches. And if that’s not enough scalability for you, DGX GH200 clusters can be further scaled up in size by using InfiniBand, which is present in the cluster as part of NVIDIA’s use of ConnectX-7 network adapters.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/18877/diagram-topology-nvlink-switch-system-dgx-gh200_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>The target market for the sizable silicon cluster is training large AI models. NVIDIA is leaning heavily on their existing hardware and toolsets in the field, combined with the sheer amount of memory and memory bandwidth a 256-node cluster affords to be able to accommodate some of the largest AI models around. The recent explosion in interest in large language models has exposed just how much memory capacity is a constraining factor, so this is NVIDIA’s attempt to offer a single-vendor, integrated solution for customers with especially large models.</p><p>And while not explicitly disclosed by NVIDIA, in a sign that they all pulling out all of the stops for the DGX GH200 cluster, the memory capacities they’ve listed indicate that NVIDIA isn’t just shipping regular H100 GPUs as part of the system, but rather they are using their limited availability 96GB models, which have the normally-disabled 6th stack of HBM3 memory enabled. So far, NVIDIA only offers these H100 variants in a handful of products – the specialty <a href=#>H100 NVL PCIe card</a> and now in some GH200 configurations – so DGX GH200 is slated to get some of NVIDIA’s best silicon.</p><p>Of course, don’t expect a supercomputer from NVIDIA to come cheaply. While NVIDIA is not announcing any pricing this far in advance, based on HGX H100 board pricing (8x H100s on a carrier board for $200K), a single DGX GH200 is easily going to cost somewhere in the low 8 digits. Suffice it to say, DGX GH200 is aimed at a rather specific subset of Enterprise clientele – those who need to do a lot of large model training and have the deep pocketbooks to pay for a complete, turn-key solution.</p><p>Ultimately, however, DGX GH200 isn’t just meant to be a high-end system for NVIDIA to sell to deep-pocketed customers, but it’s the blueprint for helping their hyperscaler customers build their own GH200-based clusters. Building such a system is, after all, the best way to demonstrate how it works and how well it works, so NVIDIA is forging their own path in this regard. And while NVIDIA would no doubt be happy to sell a whole lot of these DGX systems directly, so long as it gets hyperscalers, CSPs, and others adopting GH200 in large numbers (and not, say, rival products), then that’s still going to be a win in NVIDIA’s books.</p><p>In the meantime, for the handful of businesses that can afford a DGX GH200 AI Supercomputer, according to NVIDIA the systems will be available by the end of the year.</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH55hJZwZqeumZm2onnGq5icnV2dvLG8xKtkoZmjYrKvwMSrnJ1llqq5rXnPq6adrZOptrC6jJqlp6elo7CqusZmm6CwXZy1c3yPZpiiZaOqvaa%2Bwqikqa2kmr8%3D</p><hr><section id=social-share><div class="list-inline footer-links"><div class=share-box aria-hidden=true><ul class=share><li><a href="//twitter.com/share?url=%2fnvidia-grace-hopper-has-entered-full-production-announcing-dgx-gh200-ai-supercomputer.html&text=Grace%20Hopper%20Has%20Entered%20Full%20Production%20%26amp%3b%20Announcing%20DGX%20GH200%20AI%20Supercomputer&via=username" target=_blank title="Share on Twitter"><i class="fab fa-twitter"></i></a></li><li><a href="//www.facebook.com/sharer/sharer.php?u=%2fnvidia-grace-hopper-has-entered-full-production-announcing-dgx-gh200-ai-supercomputer.html" target=_blank title="Share on Facebook"><i class="fab fa-facebook"></i></a></li><li><a href="//reddit.com/submit?url=%2fnvidia-grace-hopper-has-entered-full-production-announcing-dgx-gh200-ai-supercomputer.html&title=Grace%20Hopper%20Has%20Entered%20Full%20Production%20%26amp%3b%20Announcing%20DGX%20GH200%20AI%20Supercomputer" target=_blank title="Share on Reddit"><i class="fab fa-reddit"></i></a></li><li><a href="//www.linkedin.com/shareArticle?url=%2fnvidia-grace-hopper-has-entered-full-production-announcing-dgx-gh200-ai-supercomputer.html&title=Grace%20Hopper%20Has%20Entered%20Full%20Production%20%26amp%3b%20Announcing%20DGX%20GH200%20AI%20Supercomputer" target=_blank title="Share on LinkedIn"><i class="fab fa-linkedin"></i></a></li><li><a href="//www.stumbleupon.com/submit?url=%2fnvidia-grace-hopper-has-entered-full-production-announcing-dgx-gh200-ai-supercomputer.html&title=Grace%20Hopper%20Has%20Entered%20Full%20Production%20%26amp%3b%20Announcing%20DGX%20GH200%20AI%20Supercomputer" target=_blank title="Share on StumbleUpon"><i class="fab fa-stumbleupon"></i></a></li><li><a href="//www.pinterest.com/pin/create/button/?url=%2fnvidia-grace-hopper-has-entered-full-production-announcing-dgx-gh200-ai-supercomputer.html&description=Grace%20Hopper%20Has%20Entered%20Full%20Production%20%26amp%3b%20Announcing%20DGX%20GH200%20AI%20Supercomputer" target=_blank title="Share on Pinterest"><i class="fab fa-pinterest"></i></a></li></ul></div></div></section></article><ul class="pager blog-pager"><li class=previous><a href=./juan-brown.html data-toggle=tooltip data-placement=top title="Juan Brown">&larr; Previous Post</a></li><li class=next><a href=./dua-lipa-rhinestone-dress-fishnet-tights.html data-toggle=tooltip data-placement=top title="Dua Lipa dazzles in custom Gucci crystal dress and fishnets">Next Post &rarr;</a></li></ul></div></div></div><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center footer-links"><li><a href title=RSS><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="credits copyright text-muted">All rights reserved
&nbsp;&bull;&nbsp;&copy;
2024
&nbsp;&bull;&nbsp;
<a href=./>PicoD</a></p><p class="credits theme-by text-muted"><a href=https://gohugo.io>Hugo v0.98.0</a> powered &nbsp;&bull;&nbsp; Theme <a href=https://github.com/halogenica/beautifulhugo>Beautiful Hugo</a> adapted from <a href=https://deanattali.com/beautiful-jekyll/>Beautiful Jekyll</a></p></div></div></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js integrity=sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js integrity=sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe crossorigin=anonymous></script>
<script src=https://code.jquery.com/jquery-1.12.4.min.js integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin=anonymous></script>
<script src=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js integrity=sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa crossorigin=anonymous></script>
<script src=https://assets.cdnweb.info/hugo/bh/js/main.js></script>
<script src=https://assets.cdnweb.info/hugo/bh/js/highlight.min.js></script>
<script>hljs.initHighlightingOnLoad()</script><script>$(document).ready(function(){$("pre.chroma").css("padding","0")})</script><script>renderMathInElement(document.body)</script><script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js integrity=sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js integrity=sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q crossorigin=anonymous></script><script src=https://assets.cdnweb.info/hugo/bh/js/load-photoswipe.js></script>
<script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/banner.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>